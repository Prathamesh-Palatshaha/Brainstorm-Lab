{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color: #f0f8ff; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); max-width: 400px; margin: auto;\">\n",
        "  <h1 style=\"color: #2a9d8f; text-align: center;\">Prathamesh Palatshaha</h1>\n",
        "  <p style=\"font-size: 1.2em; color: #333;\">\n",
        "    <strong>Roll No.: </strong><span style=\"color: #e76f51;\">21102A0022</span>\n",
        "  </p>\n",
        "  <p style=\"font-size: 1.2em; color: #333;\">\n",
        "    <strong>Division: </strong><span style=\"color: #f4a261;\">CMPN Div.A</span>\n",
        "  </p>\n",
        "  <p style=\"font-size: 1.2em; color: #333;\">\n",
        "    <strong>GitHub: </strong><a href=\"https://github.com/Prathamesh-Palatshaha/Sem-VII\" style=\"color: #264653; text-decoration: none;\">github.com/prathamesh-palatshaha</a>\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "4uNBeqWMUSBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4dON-cUHTX9v"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtBiqdi7Ttwp",
        "outputId": "8b1e401f-1364-4b63-e7d8-b01723c97d49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN-q4bgrTgl9",
        "outputId": "b504cec1-ff0b-47d9-d7c9-e8ed315c1f3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to remove HTML tags\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Function to remove extra whitespaces\n",
        "def remove_whitespace(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "# Function to remove accented characters\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Function to remove special characters\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "# Function to convert text to lowercase\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Function to remove words containing digits\n",
        "def remove_words_with_digits(text):\n",
        "    return ' '.join(word for word in text.split() if not any(char.isdigit() for char in word))\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join(word for word in nltk.word_tokenize(text) if word.lower() not in stop_words)\n",
        "\n",
        "# Function to perform lemmatization\n",
        "def lemmatize_text(text):\n",
        "    return ' '.join(lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text))"
      ],
      "metadata": {
        "id": "kOvokgrLTg7I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full preprocessing pipeline\n",
        "def preprocess_text(text):\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_whitespace(text)\n",
        "    text = remove_accented_chars(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_words_with_digits(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatize_text(text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "o0kuvDeWTha8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "<html>\n",
        "    <head><title>Sample Text</title></head>\n",
        "    <body>\n",
        "        <p>This is a sample text with numbers 123 and special characters !@#.</p>\n",
        "        <p>It's also containing some HTML tags, accented characters like café, and contractions like don't.</p>\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess the sample text\n",
        "processed_text = preprocess_text(sample_text)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhEDg7P_UCtP",
        "outputId": "93d3b00a-72b2-4ca8-8b75-93e7ffbbf1b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample text sample text number special character also containing html tag accented character like cafe contraction like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1H0nilo-UCpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dv-nGr7SUCnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b50XzdJeUClR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McZ4wo0tUCjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rjyS88SFUCg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkQmGgB1UCem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NjfkBIFUCcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eGwbxjoYUCZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZ136fVKUCXH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}